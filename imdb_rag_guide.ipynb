{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RAG Pipeline with the IMDb Dataset\n",
    "\n",
    "This notebook demonstrates how to use the existing RAG (Retrieval Augmented Generation) pipeline to load, process, and query data from the IMDb dataset. We will leverage the helper scripts available in the `app` directory for interacting with Azure OpenAI and Azure Cognitive Search.\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "1.  Configured your Azure OpenAI and Azure Cognitive Search credentials in a `.env` file in the root of this repository. The required environment variables are detailed in:\n",
    "    *   `app/openai_client.py` (for Azure OpenAI details like `AZURE_OPENAI_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_EMBEDDING_MODEL`, `AZURE_OPENAI_COMPLETION_MODEL`)\n",
    "    *   `app/search_client.py` (for Azure Cognitive Search details like `AZURE_SEARCH_ENDPOINT`, `AZURE_SEARCH_KEY`, `AZURE_SEARCH_INDEX`)\n",
    "2.  Installed all necessary Python packages. You can usually install them by running `pip install -r requirements.txt` in your terminal. This notebook might also require additional libraries like `datasets` (for Hugging Face datasets) and `pandas`, which we'll install below if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install specific libraries needed for this notebook if not already present\n",
    "# It's generally recommended to manage dependencies via requirements.txt\n",
    "# but this cell ensures these are available for notebook execution.\n",
    "try:\n",
    "    import datasets\n",
    "    import pandas\n",
    "    import fitz # PyMuPDF, used by app.ingest\n",
    "    import openai # Used by app.openai_client\n",
    "    from azure.search.documents import SearchClient # Used by app.search_client\n",
    "    from azure.core.credentials import AzureKeyCredential # Used by app.search_client\n",
    "    print(\"Required libraries are likely already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing missing libraries: datasets, pandas, PyMuPDF, openai-python, azure-search-documents, azure-core...\")\n",
    "    # Note: The notebook environment might require a kernel restart after installation.\n",
    "    %pip install datasets pandas PyMuPDF openai azure-search-documents azure-core\n",
    "    print(\"Installation complete. You might need to restart the kernel for the changes to take effect.\")\n",
    "\n",
    "# Basic imports to check if the app modules can be reached (assuming notebook is in repo root)\n",
    "# Actual usage will be in later cells.\n",
    "print(\"\\nChecking if app modules are accessible...\")\n",
    "import app.openai_client\n",
    "import app.search_client\n",
    "import app.ingest\n",
    "import app.rag_pipeline\n",
    "print(\"App modules seem accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load IMDb Dataset\n",
    "\n",
    "We'll use the Hugging Face `datasets` library to easily download and load the IMDb dataset. This dataset contains movie reviews, which we'll use as the source text for our RAG system. We'll load a small subset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the IMDb dataset from Hugging Face datasets\n",
    "print(\"Loading IMDb dataset...\")\n",
    "try:\n",
    "    # Load the 'train' split, and take only the first 1000 examples for quicker processing\n",
    "    # You can adjust the number of samples or use the full dataset if preferred.\n",
    "    imdb_dataset = load_dataset(\"imdb\", split=\"train[:1000]\") \n",
    "    print(\"IMDb dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading IMDb dataset: {e}\")\n",
    "    print(\"Please ensure you have an active internet connection and the 'datasets' library is correctly installed.\")\n",
    "    imdb_dataset = None\n",
    "\n",
    "if imdb_dataset:\n",
    "    # Convert to pandas DataFrame for easier manipulation and viewing\n",
    "    imdb_df = pd.DataFrame(imdb_dataset)\n",
    "    \n",
    "    print(\"\\nFirst 5 rows of the IMDb dataset:\")\n",
    "    print(imdb_df.head())\n",
    "    \n",
    "    print(\"\\nDataset information:\")\n",
    "    imdb_df.info()\n",
    "    \n",
    "    # We will primarily use the 'text' (review) and 'label' (sentiment) fields.\n",
    "    # For RAG, the 'text' field will be our document content.\n",
    "else:\n",
    "    print(\"\\nSkipping DataFrame creation as dataset loading failed.\")\n",
    "    imdb_df = None # Ensure imdb_df is None if dataset loading failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Ingestion\n",
    "\n",
    "Now that we have the IMDb data, we need to process it into a format suitable for our RAG pipeline. This involves:\n",
    "1.  **Chunking**: Breaking down long reviews into smaller, manageable pieces.\n",
    "2.  **Embedding**: Converting each text chunk into a numerical vector representation using an Azure OpenAI embedding model.\n",
    "3.  **Formatting**: Structuring the data (chunk, embedding, metadata) into documents for Azure Cognitive Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json # Added import for json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory (root of the repo) to the Python path\n",
    "# to allow importing from the 'app' module.\n",
    "# This is necessary if the notebook is in the root and 'app' is a subdirectory.\n",
    "notebook_dir = os.getcwd() # Should be the repo root\n",
    "if notebook_dir not in sys.path:\n",
    "    sys.path.append(notebook_dir)\n",
    "\n",
    "try:\n",
    "    from app.openai_client import get_embedding\n",
    "    from app.ingest import chunk_text, EMBEDDING_DIMENSIONS # EMBEDDING_DIMENSIONS is defined in ingest.py\n",
    "    from app.search_client import AZURE_SEARCH_INDEX_NAME # Using the default from search_client\n",
    "    print(\"Successfully imported functions from 'app' module.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from 'app' module: {e}\")\n",
    "    print(\"Please ensure the notebook is in the root directory of the repository and the 'app' module is structured correctly.\")\n",
    "    print(\"Stopping execution for this cell.\")\n",
    "    # You might want to raise an error or stop notebook execution here if imports fail\n",
    "    # For now, we'll let it proceed and subsequent cells will likely fail if imports are missing.\n",
    "    # Fallback values if imports fail, to prevent NameError later, though functionality will be broken.\n",
    "    def get_embedding(text): print(\"Error: get_embedding not loaded\"); return []\n",
    "    def chunk_text(text, **kwargs): print(\"Error: chunk_text not loaded\"); return [text]\n",
    "    EMBEDDING_DIMENSIONS = 1536 # Default, but might be wrong if not loaded\n",
    "    AZURE_SEARCH_INDEX_NAME = \"rag-vector-index\" # Default, but might be wrong\n",
    "\n",
    "# Ensure imdb_df is available from the previous step\n",
    "if 'imdb_df' not in globals() or imdb_df is None:\n",
    "    print(\"Error: imdb_df is not available. Please ensure the previous cells for loading data have run successfully.\")\n",
    "    # Stop or handle error appropriately\n",
    "    documents_to_upload = []\n",
    "else:\n",
    "    print(f\"Preparing documents for ingestion. Using embedding dimension: {EMBEDDING_DIMENSIONS}\")\n",
    "    print(f\"Target Azure Search Index Name: {AZURE_SEARCH_INDEX_NAME}\")\n",
    "\n",
    "    documents_to_upload = []\n",
    "    # Let's process a smaller subset for this example to speed up embedding generation.\n",
    "    # You can increase this number or process the whole imdb_df.\n",
    "    sample_size_for_ingestion = 50 \n",
    "    \n",
    "    # Check if imdb_df has enough rows\n",
    "    if len(imdb_df) < sample_size_for_ingestion:\n",
    "        print(f\"Warning: Requested sample size {sample_size_for_ingestion} is larger than the DataFrame size {len(imdb_df)}. Processing all available rows.\")\n",
    "        sample_df = imdb_df\n",
    "    else:\n",
    "        sample_df = imdb_df.head(sample_size_for_ingestion)\n",
    "\n",
    "    print(f\"Processing {len(sample_df)} reviews for ingestion...\")\n",
    "\n",
    "    for index, row in sample_df.iterrows():\n",
    "        review_text = row['text']\n",
    "        review_label = row['label'] # 0 for negative, 1 for positive\n",
    "\n",
    "        # Chunk the review text\n",
    "        text_chunks = chunk_text(review_text, chunk_size=1000, overlap=200)\n",
    "\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            print(f\"Processing review {index}, chunk {i+1}/{len(text_chunks)}...\")\n",
    "            embedding = get_embedding(chunk)\n",
    "\n",
    "            if not embedding:\n",
    "                print(f\"Warning: Failed to generate embedding for review {index}, chunk {i}. Skipping this chunk.\")\n",
    "                continue\n",
    "            \n",
    "            if len(embedding) != EMBEDDING_DIMENSIONS:\n",
    "                print(f\"Warning: Embedding for review {index}, chunk {i} has dimension {len(embedding)}, expected {EMBEDDING_DIMENSIONS}. Skipping chunk.\")\n",
    "                continue\n",
    "\n",
    "            # Create a unique ID for each chunk\n",
    "            # Using DataFrame index and chunk index to ensure uniqueness\n",
    "            doc_id = f\"imdb_{index}_chunk_{i}\"\n",
    "            \n",
    "            document = {\n",
    "                \"id\": doc_id,\n",
    "                \"content\": chunk,\n",
    "                \"content_vector\": embedding,\n",
    "                \"source_document_id\": f\"imdb_review_{index}\", # Original review identifier\n",
    "                \"metadata\": json.dumps({ # search_client expects metadata as a JSON string\n",
    "                    \"original_review_index\": index,\n",
    "                    \"sentiment_label\": \"positive\" if review_label == 1 else \"negative\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"text_length\": len(chunk)\n",
    "                })\n",
    "            }\n",
    "            documents_to_upload.append(document)\n",
    "            # print(f\"Prepared document ID: {doc_id} for upload.\")\n",
    "\n",
    "    print(f\"\\nTotal documents prepared for upload: {len(documents_to_upload)}\")\n",
    "    if documents_to_upload:\n",
    "        print(\"First prepared document (sample):\")\n",
    "        print(documents_to_upload[0]['id'])\n",
    "        print(documents_to_upload[0]['content'][:100] + \"...\") # Print first 100 chars of content\n",
    "        # print(documents_to_upload[0]['metadata']) # metadata can be long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on API Calls and Costs:**\n",
    "\n",
    "Generating embeddings involves making API calls to Azure OpenAI, which may incur costs depending on your subscription and the number of chunks processed. For this demonstration, we are processing a small subset of the data. If you run this on a very large dataset, be mindful of the potential cost and time involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ingest Data into Azure Cognitive Search\n",
    "\n",
    "With the data prepared, the next step is to upload it to our Azure Cognitive Search index. This will make the data searchable and retrievable by our RAG pipeline.\n",
    "\n",
    "We will:\n",
    "1.  Ensure the search index exists (and create it if it doesn't) with the correct vector configuration.\n",
    "2.  Upload the prepared documents (text chunks and their embeddings) to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary functions and variables are available\n",
    "# These should have been imported in Cell 6, but we re-affirm for clarity of this step.\n",
    "try:\n",
    "    from app.search_client import create_vector_index_if_not_exists, upload_documents, AZURE_SEARCH_INDEX_NAME\n",
    "    from app.ingest import EMBEDDING_DIMENSIONS # Or define/load EMBEDDING_DIMENSIONS if not already\n",
    "    print(\"Search client functions and variables are loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing for search client operations: {e}\")\n",
    "    print(\"Stopping execution for this cell. Please ensure previous imports were successful.\")\n",
    "    # Fallback definitions to prevent NameError, though functionality will be broken\n",
    "    def create_vector_index_if_not_exists(**kwargs): print(\"Error: create_vector_index_if_not_exists not loaded\")\n",
    "    def upload_documents(docs, **kwargs): print(\"Error: upload_documents not loaded\")\n",
    "    AZURE_SEARCH_INDEX_NAME = \"rag-vector-index\" # Default\n",
    "    EMBEDDING_DIMENSIONS = 1536 # Default\n",
    "\n",
    "if 'documents_to_upload' not in globals() or not documents_to_upload:\n",
    "    print(\"Error: `documents_to_upload` is empty or not defined. Please ensure the previous data preparation step ran successfully and produced documents.\")\n",
    "    print(\"Skipping ingestion.\")\n",
    "else:\n",
    "    print(f\"Starting data ingestion into Azure Cognitive Search index: '{AZURE_SEARCH_INDEX_NAME}'\")\n",
    "    \n",
    "    # 1. Create vector index if it doesn't exist\n",
    "    # This uses the EMBEDDING_DIMENSIONS defined in app.ingest (or imported) \n",
    "    # and AZURE_SEARCH_INDEX_NAME from app.search_client.\n",
    "    print(f\"Ensuring index '{AZURE_SEARCH_INDEX_NAME}' exists with vector dimensions {EMBEDDING_DIMENSIONS}...\")\n",
    "    try:\n",
    "        create_vector_index_if_not_exists(index_name=AZURE_SEARCH_INDEX_NAME, vector_dimensions=EMBEDDING_DIMENSIONS)\n",
    "        print(f\"Index '{AZURE_SEARCH_INDEX_NAME}' is ready.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while trying to create or verify the index: {e}\")\n",
    "        print(\"Please check your Azure Search service and configurations.\")\n",
    "        # Depending on the error, you might want to stop here.\n",
    "        # For now, we'll attempt to upload anyway, but it might fail.\n",
    "\n",
    "    # 2. Upload the documents\n",
    "    print(f\"Uploading {len(documents_to_upload)} documents to the index...\")\n",
    "    try:\n",
    "        upload_documents(documents=documents_to_upload, index_name=AZURE_SEARCH_INDEX_NAME)\n",
    "        # The upload_documents function in search_client.py should print success/failure counts.\n",
    "        print(\"Document upload process initiated.\")\n",
    "        print(\"Check the output from 'upload_documents' above for details on success/failures.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during document upload: {e}\")\n",
    "        print(\"Please ensure your Azure Search service is running and configured correctly, and that the documents are in the correct format.\")\n",
    "\n",
    "    print(\"\\nData ingestion step complete.\")\n",
    "    print(f\"The documents should now be indexed in '{AZURE_SEARCH_INDEX_NAME}'.\")\n",
    "    print(\"It might take a few moments for the indexing process to complete on Azure's side before the data is fully searchable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrate RAG Pipeline\n",
    "\n",
    "Now that the IMDb data is ingested into Azure Cognitive Search, we can use the RAG pipeline to ask questions and retrieve information. The `run_rag_pipeline` function from `app.rag_pipeline` will:\n",
    "1.  Take your query.\n",
    "2.  Generate an embedding for the query.\n",
    "3.  Search the Azure Cognitive Search index for relevant document chunks.\n",
    "4.  Construct a prompt using these retrieved chunks and your original query.\n",
    "5.  Send the prompt to an Azure OpenAI model to generate an answer.\n",
    "6.  It also includes caching for repeated queries.\n",
    "\n",
    "Let's try some example queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary function is available\n",
    "try:\n",
    "    from app.rag_pipeline import run_rag_pipeline\n",
    "    print(\"RAG pipeline function loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing RAG pipeline function: {e}\")\n",
    "    print(\"Stopping execution for this cell. Please ensure 'app.rag_pipeline' is accessible.\")\n",
    "    def run_rag_pipeline(query): \n",
    "        print(f\"Error: run_rag_pipeline not loaded. Query was: {query}\")\n",
    "        return \"RAG pipeline is not available.\"\n",
    "\n",
    "# Example Queries\n",
    "# Note: The quality of answers depends heavily on the ingested data (we only did a small sample),\n",
    "# the underlying LLM, and the effectiveness of the retrieval.\n",
    "# With only 50 reviews processed, the context might be limited.\n",
    "\n",
    "queries = [\n",
    "    \"What are people saying about movies with positive reviews?\",\n",
    "    \"Are there any mentions of 'action sequences' in the reviews?\",\n",
    "    \"Tell me about a film that someone found 'boring'.\",\n",
    "    # Add a query that might hit cache if run twice\n",
    "    \"What is a common positive sentiment expressed in reviews?\" \n",
    "]\n",
    "\n",
    "if 'run_rag_pipeline' in locals() and callable(run_rag_pipeline):\n",
    "    for query in queries:\n",
    "        print(f\"\\n--- Querying RAG pipeline ---\")\n",
    "        print(f\"User Query: {query}\")\n",
    "        \n",
    "        response = run_rag_pipeline(query) # This function handles printing from the app logic too.\n",
    "                                           # The function already prefixes with [CAG - cached] or [RAG - generated]\n",
    "        print(f\"Response:\\n{response}\")\n",
    "\n",
    "    # Example of running a query again to demonstrate caching\n",
    "    if len(queries) > 0:\n",
    "        cached_query_example = queries[-1] # Use the last query from the list\n",
    "        print(f\"\\n--- Querying RAG pipeline AGAIN (testing cache) ---\")\n",
    "        print(f\"User Query: {cached_query_example}\")\n",
    "        response = run_rag_pipeline(cached_query_example)\n",
    "        print(f\"Response:\\n{response}\")\n",
    "        print(\"Note: If the response above starts with '[CAG - cached]', it means the result was successfully retrieved from the cache.\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot run RAG pipeline demonstrations as the function is not loaded.\")\n",
    "\n",
    "print(\"\\nRAG pipeline demonstration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Queries!\n",
    "\n",
    "Feel free to modify the `queries` list in the cell above or add new cells to experiment with your own questions about the IMDb reviews you've ingested. Remember that the scope of retrievable information is limited to the subset of data processed and ingested in the earlier steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully walked through the process of using the existing RAG pipeline with a new dataset (IMDb reviews).\n",
    "\n",
    "**In this notebook, we have:**\n",
    "1.  Set up the environment and understood the prerequisites.\n",
    "2.  Loaded a sample of the IMDb dataset.\n",
    "3.  Prepared the data by chunking text and generating embeddings using Azure OpenAI.\n",
    "4.  Ingested the processed data into an Azure Cognitive Search index.\n",
    "5.  Demonstrated how to query the data using the RAG pipeline and observed its components, including caching.\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "This notebook provides a foundation. You can further explore and expand upon this by:\n",
    "*   **Experimenting with More Queries**: Try different and more complex questions to test the limits of the RAG system with the current data.\n",
    "*   **Ingesting More Data**:\n",
    "    *   Increase the `sample_size_for_ingestion` in Cell 6 to include more IMDb reviews.\n",
    "    *   Adapt the ingestion process for other text datasets you might have.\n",
    "*   **Exploring `app` Components**: Dive deeper into the Python scripts in the `app/` directory:\n",
    "    *   `app/openai_client.py`: See how Azure OpenAI services are called for embeddings and completions.\n",
    "    *   `app/search_client.py`: Understand the interaction with Azure Cognitive Search (index creation, data upload, vector search).\n",
    "    *   `app/ingest.py`: Review the data preparation and ingestion logic, which you can adapt for other data sources (e.g., different file types, databases).\n",
    "    *   `app/rag_pipeline.py`: Analyze the core RAG orchestration logic.\n",
    "*   **Modifying Parameters**:\n",
    "    *   Adjust `chunk_size` and `overlap` in `app.ingest.chunk_text` (or in Cell 6 when calling it) to see how it affects retrieval.\n",
    "    *   If you have access to different Azure OpenAI models, try changing `AZURE_OPENAI_EMBEDDING_MODEL` or `AZURE_OPENAI_COMPLETION_MODEL` in your `.env` file (and update `EMBEDDING_DIMENSIONS` in `app/ingest.py` and the notebook if your embedding model's output dimension changes).\n",
    "*   **Evaluating Performance**: For more advanced use cases, you might look into RAG evaluation frameworks (like RAGAs, for which logging is already partially implemented in `app.rag_pipeline`) to systematically assess the quality of the retrieved context and generated answers.\n",
    "\n",
    "Happy exploring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
